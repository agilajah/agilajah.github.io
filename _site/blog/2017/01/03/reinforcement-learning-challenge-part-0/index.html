<!DOCTYPE html>
<html>
    <head>
        <meta charset=utf-8>
        <meta name=viewport content="width=device-width, initial-scale=1.0">
        <meta name=description content="Abstract but still well-read">

        <title>
          
            Part 0, Reinforcement Learning Challenge - FrozenLake Environment |
          
          Febi Agil Ifdillah
        </title>

        <script src=http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML.js type=text/javascript></script>
        <script>
            MathJax.Hub.Config({
              "HTML-CSS": {
                linebreaks: {
                    automatic: true,
                    width: "70% container"
                },
                styles: {
                    ".MathJax .math": {
                        //"border": "1px solid #ccc",
                        //"margin": "0.1em 0",
                        //"padding": "0.3em",
                        //"vertical-align": "middle"
                    },
                    ".MathJax_Display .math": {
                        "border": "none",
                        "padding": "0",
                    }
                },
                scale: 100
              }
            });
        </script>

        <script type="text/javascript">
          /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
          var disqus_shortname = 'agilajah'; // required: replace example with your forum shortname
          // var disqus_developer = 1; // Comment out when the site is live

          /* * * DON'T EDIT BELOW THIS LINE * * */
          (function () {
            var s = document.createElement('script'); s.async = true;
            s.type = 'text/javascript';
            s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
          }());
        </script>
        <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-89520017-1', 'auto');
  ga('send', 'pageview');
</script>

        <link rel=stylesheet type=text/css href=/css/pure-min.css>
        <link rel=stylesheet type=text/css href=/css/github.css>
        <link rel=stylesheet type=text/css href=/css/styles.css>
        <link rel=stylesheet type=text/css href=/css/font-awesome/css/font-awesome.min.css>
        <link rel=alternate type=application/rss+xml title="RSS feed for eyeshalfclosed.com" href="/feed.xml">
    </head>
    <body>
        <div class="container pure-g-r">
            <div class=pure-u-1-4>
                <div class=author-info>
    <img src="/images/author-image.jpg" class=author-image />
    <h1 class=author-name><a href=/>Febi Agil Ifdillah</a></h1>
    <div class=nav>
        <a href="https://github.com/agilajah"><i class=icon-github-alt></i></a>
        <a href="https://twitter.com/_agilajah"><i class=icon-twitter></i></a>
        <a href="https://linkedin.com/in/febagil"><i class=icon-linkedin></a></i>
        <a href="http://feeds.feedburner.com/febiagil"><i class=icon-rss></i></a>
    </div>
    <div class=home-nav>
        <ul>
            <li><a href="/blog/">Journal</a></li>
            <li><a href="/projects/">Projects</a>
                &nbsp;|&nbsp;
                <a href="https://github.com/agilajah">Code</a>
            </li>
            <li>
                <a class=resume-link href="/cv.pdf">CV</a><br/>
                <span class=resume-update>(Updated Jan. '05)</span>
            </li>
        </ul>
    </div>
</div>

            </div>
            <div class=pure-u-3-4>
                <div class=right-column>
                    <div class=post>
    
    <ul class=post-meta>
    
    <li class=publish-time><i class=icon-calendar></i>January 03, 2017</li>
    <li class=comment-count><a href="index.html#disqus_thread" data-disqus-identifier=""></a></li>
    
        <li>&middot;</li>
        <li><a href="/tags/#x-days challenge-ref">#x-days challenge</a></li>
    
        <li>&middot;</li>
        <li><a href="/tags/#Reinforcement Learning-ref">#Reinforcement Learning</a></li>
    
        <li>&middot;</li>
        <li><a href="/tags/#Machine Learning-ref">#Machine Learning</a></li>
    
        <li>&middot;</li>
        <li><a href="/tags/#Part-0-ref">#Part-0</a></li>
    
</ul>

    <h1 class=title-large>Part 0, Reinforcement Learning Challenge - FrozenLake Environment</h1>
    <div class=content>
        <p>Di tahun yang baru ini saya memiliki beberapa program yang, harapannya, dapat menunjang proses belajar saya. Salah satunya adalah <strong>x days challenge</strong>. Di mana saya akan menantang diri saya sendiri untuk mempelajari suatu hal dalam x hari.</p>

<p>Saya tidak menspesifikasikan nilai x karena saya rasa setiap topik memiliki tingkat kesulitan tersendiri dan unik. Jika saya paksakan untuk mempelajari hal-hal (dasar) terkait topik tersebut dalam, misalnya 7 hari, 2 pekan, dan sebagainya. Saya merasa hal tersebut akan membatasi eksplorasi saya di topik tersebut. Namun, hal tersebut juga dapat membuat proses belajar saya menjadi lebih lama karena kecenderungan untuk terus mempelajari hal lain (<em>which is good</em>), tapi akan berdampak kurang baik terhadap program ini. Karena waktu yang kita miliki terbatas, dan banyak topik yang menarik yang dapat dipelajari.</p>

<p>Untuk itu, saya putuskan untuk menentukannya sembari berjalan. Jika <em>goal</em>s saya dalam mempelajari topik tersebut sudah tercapai, maka saya pikir saat itu juga <em>challenge</em> tersebut sudah selesai.</p>

<p>Untuk challenge pertama yang akan saya jalani, saya memilih topik <em>reinforcement learning</em>. Kenapa? Karena saat ini saya sedang ‘kasmaran’ dengan <em>Machine Learning</em>, dan <em>reinforcement learning</em> (RL) merupakan ‘aliran’ <em>Machine Learning</em> yang paling memikat hati saya disebabkan sifatnya yang unik dibandingkan paradigma lainnya  (<em>supervised</em>, <em>unsupervised learning</em>).</p>

<p>Pada bagian ke-0 ini (saya akan memulai dari 0), saya akan mencoba mengimplementasikan sebuah solusi dari persoalan yang diambil dari <a href="http://gym.openai.com">OpenAI Gym</a> menggunakan bahasa python. Beberapa catatan untuk mempermudah kita dalam memahami persoalan tersebut sudah saya tulis di sini, berikut dengan kode dari solusinya jika Anda ingin langsung melihat solusi: <a href="https://github.com/agilajah/xdays-reinforcementLearning/tree/master/day0-FrozenLake">click here</a>. Atau jika Anda ingin membaca lebih lanjut tentang persoalan dan bagaimana pendekatan yang digunakan untuk mencari solusi dari persoalan tersebut, silahkan untuk dibaca lebih lanjut.</p>

<h2 id="frozenlake-environment"><em>FrozenLake Environment</em></h2>

<p><img src="http://localhost:4000/images/2017/january/frozenlake.png" alt="FrozenLake Environment. Source : gym.openai.com" /></p>

<p>Lingkungan FrozenLake (FrozenLake <em>environment</em>) merupakan salah satu <em>environment</em> yang tersedia di <a href="http://gym.openai.com">OpenAI Gym</a>. Terdiri dari <em>grid</em> 4x4. Setiap block <em>grid</em> merupakan salah satu dari: <em>start block</em>, <em>goal block</em>, <em>safe frozen block</em>, dan <em>dangerous hole</em>. Sasaran kita adalah untuk membuat <em>agent</em> kita dapat bergerak dari <em>start block</em> ke <em>goal block</em> tanpa terjatuh ke lubang manapun. Namun, ada angin yang sesekali berhembus yang akan membuat <em>agent</em> tertiup ke tempat yang tidak diinginkan.</p>

<p>Bisa jadi, kita terjatuh ke lubang! Karenanya, kinerja yang sempurna dari <em>agent</em> di setiap waktu akan menjadi hal yang mustahil. Tetapi belajar untuk menghidari lubang dan mencapai <em>goal</em> masih sangat memungkinkan. <em>reward</em> yang diberikan di setiap langkah nya adalah 0, kecuali saat kita masuk ke <em>goal block</em>, di mana <em>reward</em>nya adalah 1.</p>

<h2 id="solution"><em>Solution</em></h2>

<p>Pada kasus ini, kita membutuhkan algoritma yang dapat belajar dengan ekspektasi <em>reward</em> jangka panjang (karena kita hanya akan mendapatkan <em>reward</em> saat mencapai <em>goal block</em>). Untuk itu, kita akan menggunakan Q-Learning. Karena Q-learning sangat cocok untuk diterapkan pada persoalan semacam ini.</p>

<p>Pada bentuk paling sederhana dari <em>Reinforcement Learning</em>, kita dapat menggunakan <em>lookup table</em> atau <em>arrays</em> untuk menyimpan aproksimasi dari <em>value function</em> (karena ruang kondisi dan aksi (<em>state</em> and <em>action</em> <em>spaces</em>) cukup kecil). Untuk kasus frozenlake <em>environment</em>, kita akan membuat <em>table</em> nilai untuk setiap <em>state</em> dan <em>action</em> yang mungkin pada <em>environment</em>, di mana <em>state</em> adalah <em>row</em> pada <em>table</em> dan <em>action</em> adalah <em>column</em> pada <em>table</em>.</p>

<p>Ada 16 <em>state</em> yang mungkin pada lingkungan frozenlake, dan 4 buah aksi yang mungkin pada setiap <em>state</em> tersebut. Sehingga ukuran <em>table</em> untuk menyimpan aproksimasi nilai dari <em>value function</em> tersebut adalah 16x4.</p>

<p>Kita akan meng<em>update</em> Q-<em>table</em> menggunakan persamaan Bellman yang menyatakan bahwa ‘<em>long-term</em> <em>reward</em> yang diharapkan untuk aksi yang diberikan sama dengan <em>immediate reward</em> dari aksi saat ini dikombinasikan dengan <em>reward</em> yang diharapkan dari aksi masa depan yang paling baik yang diambil di <em>state</em> berikutnya’ yang dapat kita tuliskan dengan persamaan berikut :</p>

<p><code class="highlighter-rouge">Q(s, a) = r + γ (max(Q (s', a')))</code></p>

<p>di mana (s) adalah <em>state</em> saat itu, (a) adalah aksi pada <em>state</em> tersebut, (r) adalah <em>reward</em>, (y) adalah discount-rate parameter, (s’) adalah <em>state</em> selanjutnya, dan (a’) adalah aksi berikutnya pada (s’). Discount-rate parameter digunakan untuk mempertimbangkan sebarapa penting kah <em>reward</em> di masa mendatang jika dibandingkan dengan <em>reward</em> saat ini.</p>

<p>Dengan meng<em>update</em> <em>table</em> dengan cara seperti ini, <em>table</em> tersebut perlahan-lahan akan berisikan perhitungan yang cukup akurat terhadap <em>reward</em> di masa yang akan datang untuk <em>state</em> saat ini dan aksi di <em>state</em> tersebut.</p>

<p>Apabila kita tinjau algoritma Q dalam konteks <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a>, maka <code class="highlighter-rouge">r + γ(max(Q(s’,a’))</code> adalah hal yang ingin coba kita capai. Tapi kita tahu bahwa <code class="highlighter-rouge">r + γ(max(Q(s’,a’))</code> merupakan estimasi dengan <em>noise</em> dari nilai Q yang sebenarnya pada area tersebut. Jadi, kita tidak akan meng<em>update</em> <em>Q-table</em> menggunakan persamaan tersebut melainkan menambahkan sedikit langkah untuk membuat nilai Q mendekati yang kita inginkan. Maka kita gunakan <code class="highlighter-rouge">Q[s,a] ←Q[s,a] + α(r+ γ * max Q[s',a'] - Q[s,a])</code> atau <code class="highlighter-rouge">Q[s,a] ←(1-α) Q[s,a] + α(r+ γ * max Q[s',a'])</code> di mana <code class="highlighter-rouge">α</code> adalah <em>step-size parameter</em> yang memengaruhi kecepatan belajar, atau dapat dikatakan ‘<em>learning rate</em>’, dan <code class="highlighter-rouge">γ</code> adalah <em>discount-rate parameter</em>.</p>


    </div>
    <div class="share-page">
    Share this on &rarr;
    <a href="https://twitter.com/intent/tweet?text=Part 0, Reinforcement Learning Challenge - FrozenLake Environment&url=http://localhost:4000/blog/2017/01/03/reinforcement-learning-challenge-part-0/&via=&related=" rel="nofollow" target="_blank" title="Share on Twitter">Twitter</a>
    <a href="https://facebook.com/sharer.php?u=http://localhost:4000/blog/2017/01/03/reinforcement-learning-challenge-part-0/" rel="nofollow" target="_blank" title="Share on Facebook">Facebook</a>
    <a href="https://plus.google.com/share?url=http://localhost:4000/blog/2017/01/03/reinforcement-learning-challenge-part-0/" rel="nofollow" target="_blank" title="Share on Google+">Google+</a>
</div>



    
            <div id="disqus_thread"></div>
              <script type="text/javascript">
                  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
                  var disqus_shortname = 'agilajah'; // required: replace example with your forum shortname
                  // var disqus_developer = 1; // Comment out when the site is live
                  var disqus_identifier = "http://www.febiagil.com/" + "/blog/2017/01/03/reinforcement-learning-challenge-part-0/";

                  /* * * DON'T EDIT BELOW THIS LINE * * */
                  (function() {
                      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
                  })();
              </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    

</div>

<div class="PageNavigation">
  
    <a class="prev" href="/blog/2016/12/30/steve-jobs-was-right/">&laquo; Steve Jobs Was Right</a>
  
  
    <a class="next" href="/blog/2017/01/05/reinforcement-learning-challenge-part-1/">Part 1, Reinforcement Learning Challenge - Multi-armed Bandits Problem &raquo;</a>
  
</div>

                </div>
            </div>
        </div>
    </body>
</html>
